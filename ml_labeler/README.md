# ML labelling

ML labelling is a tool to label lists of words to a given taxonomy. 

The core of the automatic labelling process is an adaptation of code taken from
the [NETL library](https://github.com/sb1992/NETL-Automatic-Topic-Labelling-)
with apache license 2.0 (details about the code taken and adapted from NETL will be given below).

The tool is oriented to process the specific files generated for the **edma challenge**, with the goal to label lists according to two specific target vocabularies: **EuroSciVoc** and **AgroVoc**. However, the core of the classification process, contained in script **`my_supervised_labels.py`** can be applied to other data sources provided that the inputs have the required format. Also, the tool can be easily extended to work with other vocabularies, althoug some extra coding would be required.

## 1. Scripts:

The tool includes three executable python scripts:

* **`bow2tax.py`**: It labels list of documents given by a tfidf representation.
* **`map2tax.py`**: It labels the files generated by the topic modeling software: topics, documents and authors
* **`get_agro.py`**: It extracts the target labels from the AgroVoc vocabulary.

## 2. Requirements:

To run the scripts, a source data folder should exist, with the following structure:

    source
    |- netl_support_files
    |    |- page_rank_titles_sorted.txt
    |    |- svm_model
    |    |- svm_rank_classify
    |- taxonomy
    |    |- EuroSciVoc_Category-All-Report.xlsx
    |    |- agrovocab.yml
    |- model
    |   |- BASE_WDcorpus_tfidf
    |   |- EXT_WDcorpus_tfidf
    |-wikisources
         |-enwiki-20200801-all-titles-in-ns0

### 2.1. NETL support files

Files `svm_model` and `svm_rank_classif` can be taken from the [SVMrank](http://www.cs.cornell.edu/people/tj/svm_light/svm_rank.html) site. Download the source files according to your OS. Binaries are available for

 * [Linux (32-bit)](http://download.joachims.org/svm_rank/current/svm_rank_linux32.tar.gz)
 * [Linux (64-bit)](http://download.joachims.org/svm_rank/current/svm_rank_linux64.tar.gz)
 * [Cygwin](http://download.joachims.org/svm_rank/current/svm_rank_cygwin.tar.gz)
 * [Windows](http://download.joachims.org/svm_rank/current/svm_rank_windows.zip)

The pre-computed page rank file `page_rank_titles_sorted.txt` can be taken from [here](https://unimelbcloud-my.sharepoint.com/:u:/g/personal/jeyhan_lau_unimelb_edu_au/ESZ-eZy0HCRFs2GbWASy41sBdMjrpjgE9-d2cpTqPdJDbQ?e=tIX7bZ).

### 2.2. Taxonomy files

The [data](https://github.com/Orieus/map2tax/blob/master/data) subfolder from the code should contain files [map_agr2wiki.yml](https://github.com/Orieus/map2tax/blob/master/data/map_agr2wiki.yml) and [map_esv2wiki.yml](https://github.com/Orieus/map2tax/blob/master/data/map_esv2wiki.yml). These files contain one-to-one map from each target taxonomy (EuroSciVoc or AgroVoc) to the wikipedia categories used by the NETL-based classifier.

When these file are not available, they can be computed. The `taxonomy` folder contains the files required to compute them.

You need the file corresponding to the selected taxonomy (`scv` or `agr` in the `--tax` option)

 * EuroSciVoc: `EuroSciVoc_Category_All-Report.xlsx` can be taken from (?)
 * AgroVoc: file `agrovocab.yml` is generated using the **`get_agro.py`** script.
 
### 2.3. Models

Model is the folder that contain the inputs to the classifier. Each script `bow2tax.py` and `map2tax.py`, require a different file structure, as will be explained below.

### 2.4. Wikisources

The file [enwiki-20200801-all-titles-in-ns0](https://dumps.wikimedia.org/enwiki/20200801/enwiki-20200801-all-titles-in-ns0.gz) is optional, but it is useful to reduce the number of queries to the wikipedia site during the computation of mappings from the target categories to the wikipedia categories used by NETL.

## 3. Usage

The following sections explain the use of each script:

### 3.1. Using `bow2tax.py`

The execution format is

    bow2tax.py [-h] --source SOURCE --output OUTPUT --model MODEL
                       [--tax TAX] [--path2tax PATH2TAX] [--tmax TMAX]

    Arguments:
      -h, --help           show this help message and exit
      --source SOURCE      path to the source data folder
      --model MODEL        path to the model to process (relative to SOURCE)
      --output OUTPUT      path to the output folder
      --tax TAX            taxonomy (esv or agr). 
                               esv: to label according to EuroSciVoc
                               agr:, to label according to AgroVoc
      --path2tax PATH2TAX  path to the file containing the taxonomy (relative
                           to source)
      --tmax TMAX          maximum number of terms to take from each document
                           (values around 10-20 could to be fine)

#### Input model:

To run the code, the source data folder should contain one or several model folders, each one of them with two files:

    |- model
        |- BASE_WDcorpus_tfidf
        |- EXT_WDcorpus_tfidf

You can store several model folders in the source folder. You just need to specify which one to use as input of each execution using the option `-model`.

#### Outputs

The output files are stored in the specified output folder. This folder will contain four files two with the labels and two with the tokens of each document used to compute the labels:

    lab_base.csv
    lab_ext.csv
    tok_base.csv
    tok_ext.csv

### 3.2 Using `map2tax.py`

The execution format is

    map2tax.py [-h] --source SOURCE --output OUTPUT --model MODEL
                       [--tax TAX] [--path2tax PATH2TAX] [--tmax TMAX]

    Arguments:
      -h, --help           show this help message and exit
      --source SOURCE      path to the source data folder
      --model MODEL        path to the model to process (relative to SOURCE)
      --output OUTPUT      path to the output folder
      --tax TAX            taxonomy (esv or agr). 
                               esv: to label according to EuroSciVoc
                               agr:, to label according to AgroVoc
      --path2tax PATH2TAX  path to the file containing the taxonomy (relative
                           to source)
      --tmax TMAX          maximum number of terms to take from each document
                           (values around 10-20 could to be fine)

#### Input model:

To run the code, the source data folder should contain one or several model folders, each one of them with two files:

    |- model
        |- word-topic-counts.txt
        |- author_topics.txt
        |- paper_topics.txt

You can store several model folders in the source folder. You just need to specify which one to use as input of each execution using the option `-model`.

#### Outputs

The output files are stored in the specified output folder. It will contain 6 files, 3 with the labels and 3 with the tokens of each document used to compute the labels:

    lab_topics.csv
    lab_docs.csv
    lab_authors.csv
    tok_topics.csv
    tok_docs.csv
    tok_authors.csv

### 3.3 Using `get_agro.py`

A we have noted above, in order to compute labels from the AgroVoc taxonomy, a file `agrovocab.yml` could be required.

    usage: get_agro.py [-h] [--source SOURCE] [--output OUTPUT]

    optional arguments:
      -h, --help       show this help message and exit 
      --source SOURCE  path to the source NT file
      --output OUTPUT  path to the output yaml file

The source NT file can be taken from [here](http://agrovoc.uniroma2.it/agrovocReleases/agrovoc_2020-07-08_lod.nt.zip)

The output is a yaml file with the categories extracted from AgroVoc. This file must be placed in the `taxonomy` folder, with the name `agrovocab.yml`, to be used by `map2tax.py` and `bow2tax.py`, if needed, to compute the wikipedia mappings.

## 4. Library usage

Script `bow2tax.py` and `map2tax.py` are specifically oriented to the data files used for the edma-challenge, but other similar scripts could be written to apply the classifier to other data sources.

The main classes and methods of the labeling tool are found in [my_supervised_labels.py](https://github.com/Orieus/map2tax/blob/master/my_supervised_labels.py).

### Classes

There are two main classes

 * **`SupLabeler`**, to classify with EuroSciVoc.
 * **`AGRLabeler`**, to classify with AgroVoc
 
The main method in charge of the automatic labeling is **`get_labels()`**. For instance, to compute EuroSciVoc labels, you can do...
    
    sl = my_supervised_labels.SupLabeler()
    sl.get_labels(
        num_sup_labels, pagerank_model, data, path2tax, svm_classify,
        pretrained_svm_model, out_sup, load_map, p2wikifile)

    """
    Parameters
    ----------
    num_sup_labels: int
        num of supervised labels needed.
    pagerank_model: str
        path to the pagerank file
    data: str
        path to the topic data file.
    output_candidates: str
        path of generated candidate file.
    svm_classify: str
        path to the SVM Ranker classify binary file.
    trained_svm_model:
        This is the pre existing trained SVM model, trained on our SVM model.
    path2tax: str
        Output file for supervised labels
    load_map: bool, optional (default=False)
        If True, the label map is loaded from path2wikimap (if provided)
        If False, the label map is computed, and saved into path2wikimap
        (if provided)
    """

### Other taxonomies:

**`SupLabeler`** is the root class. **`AGRLabeler`** is an inherited class, that adapts SupLabeler to the AgroVoc vocabulary. If you want to use the code with other taxonomies, you may need to create your own inherited classes. You will likely need to write specific versions of two methods

 * **`read_candidates(self, path2tax)`**: to read the source vocabulary of the new taxonomy
 * **`get_map2wiki(self, cats, page_rank_dict)`**: to map the categories of the new taxonomy into the wikipedia categories used by NETL. This map should be one-to-one and invertible. In practice, not all categories from the taxonomy can be maped to wikipedia, and they are discarded.
 

## 5. NETL.

This code is an adaptation of script **`supervised_labels.py`** and related code, taken from the NETL library. with apache license 2.0.

The original code is at (https://github.com/sb1992/NETL-Automatic-Topic-Labelling-)

 * Author:         Shraey Bhatia
   * Date:           October 2016
   * File: 		supervised_labels.py

 * Updated by:     Sihwa Park
   * Date:           January 7, 2019
   * Fix:            Updated to work with Python 3.6.5

The original code has been modified to provide some class structure, to modify the input and output data structure and also to speedup execution, specially for large vocabularies and documents.

Only one method has been taken without modifications:

    def get_topic_lt(self, elem):

The following was adapted to modify the format of the outpus.

    def get_predictions(self, test_set, num, svm_classify, trained_svm_model,

Other methods have been modified for higher speed, though the computing steps are essentially the same:

    def get_lt_ranks(self, topic_list, lab_list, num):
    def prepare_features(self, letter_tg_dict, page_rank_dict, cols,
                         feature_names, topic_list, categories):
    def convert_dataset(self, test_file, feature_names):

The main method, **`get_labels(...)`** has been adapted to changes the input and output data structure, but the sequence of calls to the processing methods is the same.

Finally, the following methods are original:

    def read_candidates(self, path2tax):
    def wikitest(self, cat, page_rank_dict):
    def get_map2wiki(self, cats, page_rank_dict):



 






